{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_seed = 1300\n",
    "n_seeds = 10\n",
    "\n",
    "model_id = 'lv'\n",
    "save_path = 'results/' + model_id\n",
    "\n",
    "use_maf = False\n",
    "\n",
    "noise_sd = 50.0  # compare to log(noise_sd) = 2.3 in Owen et al. 2014, Likelihood free inference for Makov Processes: a comparison\n",
    "\n",
    "include_initial_state = True\n",
    "\n",
    "# simulation setup\n",
    "setup_opts = {        \n",
    "    'n_hiddens': [50, 50],\n",
    "    'reg_lambda': 0.01,\n",
    "    'pilot_samples': 1000,\n",
    "    'verbose': True,\n",
    "    'prior_norm': False,    \n",
    "    'svi': False,\n",
    "    'n_rnn': 100,\n",
    "    'n_inputs_rnn': 2\n",
    "}\n",
    "\n",
    "run_opts = {\n",
    "    'n_train': 1000,\n",
    "    'n_rounds': 10,\n",
    "    'minibatch': 100,\n",
    "    'epochs': 2000,\n",
    "    'moo': 'resample',\n",
    "    'proposal': 'gaussian',\n",
    "    'n_null': None,\n",
    "    'train_on_all': True,\n",
    "    'max_norm': 0.1,\n",
    "    'val_frac': 0.1,\n",
    "    'silent_fail': False,\n",
    "    'reuse_prior_samples': True,\n",
    "}\n",
    "  \n",
    "if run_opts['train_on_all']:\n",
    "    run_opts['epochs'] = [run_opts['epochs'] // (r+1) for r in range(run_opts['n_rounds'])]\n",
    "\n",
    "if use_maf:\n",
    "    # control MAF seed\n",
    "    rng_maf = np.random\n",
    "    \n",
    "    setup_opts.update({\n",
    "        'mode': 'random',\n",
    "        'n_mades': 5,\n",
    "        'rng': rng_maf,\n",
    "        'act_fun': 'tanh',\n",
    "        'batch_norm': False\n",
    "    })\n",
    "\n",
    "pars_true = np.array([-4.60517019, -0.69314718,  0.        , -4.60517019])  # from SNL paper\n",
    "#x0 was generated by simulation using the true parameters from the SNL paper, initial state not removed, dt=0.2\n",
    "raw_data = np.array([50., 100., 59., 104.,  66., 117.,  81., 133.,  97., 131., 109., 135., 127.,\n",
    "       132., 148., 135., 163., 125., 188., 108., 217.,  95., 239.,  75.,\n",
    "       236.,  60., 235.,  46., 228.,  34., 217.,  28., 211.,  18., 193.,\n",
    "        12., 175.,   8., 171.,   9., 162.,   5., 151.,   5., 136.,   6.,\n",
    "       128.,   6., 111.,   8., 110.,   8.,  96.,   8.,  95.,   9.,  93.,\n",
    "         9.,  81.,  10.,  80.,  11.,  78.,  17.,  74.,  17.,  72.,  19.,\n",
    "        66.,  20.,  67.,  25.,  64.,  29.,  54.,  30.,  55.,  45.,  54.,\n",
    "        45.,  53.,  48.,  48.,  50.,  54.,  65.,  61.,  75.,  64.,  77.,\n",
    "        65.,  95.,  73., 106.,  85., 107.,  88., 114., 104., 113., 130.,\n",
    "       114., 159., 110., 173.,  95., 196.,  77., 220.,  72., 219.,  49.,\n",
    "       212.,  33., 209.,  24., 196.,  18., 188.,  18., 176.,  17., 166.,\n",
    "        15., 158.,  14., 154.,  10., 138.,  13., 128.,  11., 119.,  11.,\n",
    "       107.,   9.,  99.,   8.,  94.,  11.,  86.,  11.,  81.,  12.,  78.,\n",
    "        15.,  67.,  19.,  63.,  16.,  58.,  20.,  58.,  28.,  57.,  36.,\n",
    "        53.,  39.,  55.,  44.,  53.,  44.,  55.,  44.,  54.,  49.,  54.,\n",
    "        66.,  54.,  77.,  53.,  93.,  64.,  98.,  76., 100.,  94., 110.,\n",
    "       116., 112., 136., 102., 149., 105., 173., 103., 200.,  95., 208.,\n",
    "        69., 224.,  54., 232.,  47., 217.,  40., 206.,  29., 202.,  24.,\n",
    "       196.,  24., 181.,  23., 167.,  13., 158.,  14., 147.,   9., 135.,\n",
    "         8., 121.,   9., 111.,   8.,  95.,  10.,  84.,  10.,  73.,  11.,\n",
    "        68.,  14.,  61.,  16.,  57.,  20.,  50.,  19.,  46.,  25.,  43.,\n",
    "        33.,  41.,  38.,  42.,  40.,  45.,  50.,  48.,  57.,  48.,  60.,\n",
    "        47.,  63.,  49.,  68.,  48.,  66.,  42.,  71.,  43.,  84.,  46.,\n",
    "        81.,  40.,  89.,  45., 121.,  55., 134.,  67., 136.,  83., 135.,\n",
    "       100., 142., 129., 135., 148., 134., 163., 130., 182., 111., 195.,\n",
    "       106., 206.,  93., 216.,  67., 224.,  47., 207.,  33., 202.,  24.,\n",
    "       193.,  20., 182.,  14., 174.,   8., 161.,   8., 152.,   4., 140.,\n",
    "         6., 131.,   5.])\n",
    "raw_data += np.random.RandomState(seed=seed).randn(*raw_data.shape) * noise_sd\n",
    "obs = raw_data if include_initial_state else raw_data[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit\n",
    "from copy import deepcopy\n",
    "from delfi.utils.progress import no_tqdm, progressbar\n",
    "\n",
    "from delfi.utils.viz import plot_pdf\n",
    "import delfi.inference as infer\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator\n",
    "from delfi.summarystats import Identity\n",
    "\n",
    "from lfimodels.snl_exps.util import save_results, load_results, StubbornGenerator, stubborn_defaultrej\n",
    "from lfimodels.snl_exps.util import  save_results_byname, load_results_byname\n",
    "from lfimodels.snl_exps.util import init_g_lv as init_g\n",
    "from lfimodels.snl_exps.util import load_setup_lv as load_setup\n",
    "from lfimodels.snl_exps.util import load_gt_lv as load_gt\n",
    "from lfimodels.snl_exps.util import calc_all_lprob_errs\n",
    "from lfimodels.snl_exps.LotkaVolterra import LotkaVolterra, LotkaVolterraStats\n",
    "import snl.simulators.lotka_volterra as sim_lv\n",
    "\n",
    "print('pars_true : ', pars_true)\n",
    "\n",
    "dt = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RNN with no rejection based on summary stats, as we don't have any\n",
    "setup_opts_rnn = deepcopy(setup_opts)\n",
    "setup_opts_rnn['n_rnn'] = 100\n",
    "\n",
    "def init_all_rnn(seed, n_steps_used=(raw_data.shape[0] - 2)// 2):\n",
    "    assert (n_steps_used + int(include_initial_state)) * 2 <= raw_data.shape[0]\n",
    "    model = LotkaVolterra(dt=dt, duration=dt * n_steps_used, seed=seed, max_n_steps=50000, noise_sd=noise_sd)\n",
    "    prior = dd.Uniform(lower= [-5,-5,-5,-5], upper = [2,2,2,2], seed=seed)\n",
    "    summary = Identity(seed=seed)\n",
    "    g = StubbornGenerator(model=model, prior=prior, summary=summary, seed=seed)\n",
    "    \n",
    "    if use_maf:\n",
    "        setup_opts['rng'].seed(seed)\n",
    "    # initialize inference object\n",
    "    ii_max = n_steps_used + 1 if include_initial_state else n_steps_used\n",
    "    res = infer.SNPEC(g, obs=obs[:ii_max * 2], seed=seed, **setup_opts_rnn)\n",
    "    return g, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L, TD, P = [], [], []\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    seed = base_seed + i\n",
    "    exp_id_nostats = 'rnn_noise_seed' + str(seed)\n",
    "\n",
    "    g, res = init_all_rnn(seed=seed)\n",
    "    # train\n",
    "    t = timeit.time.time()\n",
    "    logs, tds, posteriors = res.run(**run_opts, verbose=False)\n",
    "    L.append(logs)\n",
    "    TD.append(tds)\n",
    "    P.append(posteriors)\n",
    "    print('fitting time: ', timeit.time.time() - t)\n",
    "    \n",
    "    save_results_byname(logs=logs, tds=tds, posteriors=posteriors, raw_data=raw_data,\n",
    "                        setup_opts=setup_opts_rnn, run_opts=run_opts, exp_id=exp_id_nostats, path=save_path)\n",
    "\n",
    "L_rnn_norej, TD_rnn_norej, P_rnn_norej = L, TD, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with stats, including the first time point so rejection rate is very low\n",
    "setup_opts_stats = deepcopy(setup_opts)\n",
    "setup_opts_stats['n_rnn'] = 0\n",
    "\n",
    "def init_all_stats(seed, n_steps_used=(raw_data.shape[0] - 2)// 2):\n",
    "    assert (n_steps_used + int(include_initial_state)) * 2 <= raw_data.shape[0]\n",
    "    model = LotkaVolterra(dt=dt, duration=dt * n_steps_used, seed=seed, noise_sd=noise_sd)\n",
    "    prior = dd.Uniform(lower= [-5,-5,-5,-5], upper = [2,2,2,2], seed=seed)\n",
    "    summary = LotkaVolterraStats(seed=seed)\n",
    "    g = StubbornGenerator(model=model, prior=prior, summary=summary, seed=seed)\n",
    "    \n",
    "    if use_maf:\n",
    "        setup_opts['rng'].seed(seed)\n",
    "    # initialize inference object\n",
    "    ii_max = n_steps_used + 1 if include_initial_state else n_steps_used\n",
    "    obs_stats = summary.calc([dict(data=obs[:ii_max * 2])])\n",
    "    res = infer.SNPEC(g, obs=obs_stats, seed=seed, **setup_opts_stats)\n",
    "    return g, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L, TD, P = [], [], []\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    seed = base_seed + i\n",
    "    exp_id_stats = 'stats_fc_noise_seed' + str(seed)\n",
    "    \n",
    "    g, res = init_all_stats(seed=seed)\n",
    "    # train\n",
    "    t = timeit.time.time()\n",
    "    logs, tds, posteriors = res.run(**run_opts, verbose=False)\n",
    "    L.append(logs)\n",
    "    TD.append(tds)\n",
    "    P.append(posteriors)\n",
    "    print('fitting time: ', timeit.time.time() - t)\n",
    "    \n",
    "    save_results_byname(logs=logs, tds=tds, posteriors=posteriors, raw_data=raw_data,\n",
    "                        setup_opts=setup_opts_stats, run_opts=run_opts, exp_id=exp_id_stats, path=save_path)\n",
    "\n",
    "L_stats, TD_stats, P_stats = L, TD, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstats, Vstats = load_results_byname(exp_id=exp_id_stats, path=save_path)\n",
    "L_stats = Vstats['logs']\n",
    "del Vstats\n",
    "\n",
    "pnostats, Vnostats = load_results_byname(exp_id=exp_id_nostats, path=save_path)\n",
    "L_nostats = Vnostats['logs']\n",
    "del Vnostats\n",
    "\n",
    "dim_param = pstats[0][0].ndim\n",
    "Ts = np.array(data_lengths, dtype=float) * dt\n",
    "nsamples = run_opts['n_train'] * (np.arange(run_opts['n_rounds']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = np.arange(1,16)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(dim_param):\n",
    "    _ = plt.figure()\n",
    "    i = -1\n",
    "    means_stats = np.array([pp.xs[0].m[k] for pp in pstats[i]])\n",
    "    sds_stats = np.array([np.sqrt(pp.xs[0].S[k, k]) for pp in pstats[i]])\n",
    "    means_nostats = np.array([pp.xs[0].m[k] for pp in pnostats[i]])\n",
    "    sds_nostats = np.array([np.sqrt(pp.xs[0].S[k, k]) for pp in pnostats[i]])\n",
    "    \n",
    "    plt.plot(nsamples, means_stats, 'b', label='suff. stats')\n",
    "    plt.plot(nsamples, means_stats + sds_stats, 'b:')\n",
    "    plt.plot(nsamples, means_stats - sds_stats, 'b:')\n",
    "    plt.plot(nsamples, means_nostats, 'g', label='RNN')\n",
    "    plt.plot(nsamples, means_nostats + sds_nostats, 'g:')\n",
    "    plt.plot(nsamples, means_nostats - sds_nostats, 'g:')\n",
    "    plt.plot(plt.xlim(), np.ones(2) * pars_true[k],'k')\n",
    "    plt.ylim(pars_true[k] + np.array([-1., 1.]) * 5.)\n",
    "    plt.xlim([0, 10000])\n",
    "    plt.xlabel('training samples')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "plt.plot(np.arange(len(pstats[i])) + 1, [-pp.eval(pars_true.reshape(1, -1), log=True) for pp in pstats[i]],'b')\n",
    "plt.plot(np.arange(len(pnostats[i])) + 1, [-pp.eval(pars_true.reshape(1, -1), log=True) for pp in pnostats[i]],'g')    \n",
    "plt.ylim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(dim_param):\n",
    "    _ = plt.figure()\n",
    "    iround = 3\n",
    "    means_stats = np.array([P[iround].xs[0].m[k] for P in pstats])\n",
    "    sds_stats = np.array([np.sqrt(P[iround].xs[0].S[k, k]) for P in pstats])\n",
    "    means_nostats = np.array([P[iround].xs[0].m[k] for P in pnostats])\n",
    "    sds_nostats = np.array([np.sqrt(P[iround].xs[0].S[k, k]) for P in pnostats])\n",
    "    plt.plot(Ts, means_stats, 'b')\n",
    "    plt.plot(Ts, means_stats + sds_stats, 'b:')\n",
    "    plt.plot(Ts, means_stats - sds_stats, 'b:')\n",
    "    plt.plot(Ts, means_nostats, 'g')\n",
    "    plt.plot(Ts, means_nostats + sds_nostats, 'g:')\n",
    "    plt.plot(Ts, means_nostats - sds_nostats, 'g:')\n",
    "    plt.plot(plt.xlim(), np.ones(2) * pars_true[k],'k')\n",
    "    plt.ylim(pars_true[k] + np.array([-1., 1.]) * .5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(dim_param):\n",
    "    _ = plt.figure()\n",
    "    for i, dl in enumerate(data_lengths):\n",
    "        means = np.array([pp.xs[0].m[k] for pp in pstats[i]])\n",
    "        sds = np.array([np.sqrt(pp.xs[0].S[k, k]) for pp in pstats[i]])\n",
    "        _ = plt.plot(nsamples, means,\n",
    "                     label='T = {0}'.format(Ts[i]))\n",
    "    plt.legend()\n",
    "    plt.plot(plt.xlim(), np.ones(2) * pars_true[k],'k')\n",
    "    plt.xlim([0, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for i, dl in enumerate(data_lengths):\n",
    "    h.append(plt.plot(np.arange(len(L[i])) + 1,\n",
    "                      [-pp.eval(pars_true.reshape(1, -1), log=True) for pp in P[i]],\n",
    "                     label='T = {0}'.format(dl * dt))[0])\n",
    "plt.legend(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lengths = [5, 10, 25, 60, 100, 150]\n",
    "plt.semilogx(data_lengths, np.ones_like(data_lengths),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(P[-1][-1].xs[0].S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=-1\n",
    "posteriors_C = P[i]\n",
    "for r in range(len(posteriors_C)):\n",
    "    \n",
    "    posterior_C = posteriors_C[r]\n",
    "    #posterior_C.ndim = posterior_A.ndim\n",
    "    \n",
    "    g2 = deepcopy(g)\n",
    "    g2.proposal = posterior_C\n",
    "    samples = np.array(g2.draw_params(5000)) \n",
    "    \n",
    "    fig,_ = plot_pdf(dd.Gaussian(m=0.00000123*np.ones(pars_true.size), S=1e-30*np.eye(pars_true.size)), \n",
    "                   samples=samples.T,\n",
    "                   gt=pars_true, \n",
    "                   lims=[[-5,2],[-5,2],[-5,2],[-5,2]],\n",
    "                   #lims=[0,10],\n",
    "                   resolution=100,\n",
    "                   ticks=True,\n",
    "                   figsize=(16,16));\n",
    "    \n",
    "    fig.suptitle('SNPE-C posterior estimates, round r = '+str(r+1), fontsize=14)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(posteriors_C)) + 1, [-posterior_C.eval(pars_true.reshape(1, -1), log=True) for posterior_C in posteriors_C])\n",
    "plt.ylabel('-log p of true params')\n",
    "plt.xlabel('round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1\n",
    "posteriors_C = P[i]\n",
    "posterior_C = res.predict(res.obs)\n",
    "posterior_C.xs[0].S[3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    #posterior_C.ndim = posterior_A.ndim\n",
    "    \n",
    "    g2 = deepcopy(g)\n",
    "    g2.proposal = posterior_C\n",
    "    samples = np.array(g2.draw_params(5000)) \n",
    "    \n",
    "    fig,_ = plot_pdf(dd.Gaussian(m=0.00000123*np.ones(pars_true.size), S=1e-30*np.eye(pars_true.size)), \n",
    "                   samples=samples.T,\n",
    "                   gt=pars_true, \n",
    "                   lims=[[-5,2],[-5,2],[-5,2],[-5,2]],\n",
    "                   #lims=[0,10],\n",
    "                   resolution=100,\n",
    "                   ticks=True,\n",
    "                   figsize=(16,16));\n",
    "    \n",
    "    fig.suptitle('SNPE-C posterior estimates, round r = '+str(r+1), fontsize=14)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
